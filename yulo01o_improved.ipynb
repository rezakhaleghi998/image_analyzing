{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Object Detection for Image Analysis\n",
    "\n",
    "This notebook demonstrates the use of YOLO (You Only Look Once) for object detection in images. We'll use the YOLOv8 model to detect objects in various images and analyze the results.\n",
    "\n",
    "## Overview\n",
    "1. Setup and Dependencies\n",
    "2. Loading YOLOv8 Model\n",
    "3. Object Detection on Sample Images\n",
    "4. Analyzing and Visualizing Results\n",
    "5. Advanced Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, we'll install the necessary libraries and import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install ultralytics\n",
    "!pip install supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "\n",
    "# Set display parameters for better visualization\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 10)\n",
    "plt.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading YOLOv8 Model\n",
    "\n",
    "Here we load the pre-trained YOLOv8 model. YOLOv8 is a state-of-the-art object detection model that offers an excellent balance between speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load a pre-trained YOLO model (YOLOv8n)\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"Model loaded: {model.info()}\")\n",
    "print(f\"Model task: {model.task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Object Detection on Sample Images\n",
    "\n",
    "Now we'll perform object detection on sample images from different sources. We'll start with a basic example and then analyze more complex images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Basic Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample image URL\n",
    "IMAGE_URL = \"https://ultralytics.com/images/bus.jpg\"\n",
    "\n",
    "# Download and display the original image\n",
    "!wget -q -O bus.jpg {IMAGE_URL}\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(12, 8))\n",
    "img = plt.imread(\"bus.jpg\")\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original Image\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform object detection on the image\n",
    "results = model(\"bus.jpg\")[0]\n",
    "\n",
    "# Display the image with detections\n",
    "detections = sv.Detections.from_yolov8(results)\n",
    "bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "# Read the image with OpenCV for annotation\n",
    "img = cv2.imread(\"bus.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "# Annotate the image\n",
    "annotated_image = bounding_box_annotator.annotate(img, detections)\n",
    "annotated_image = label_annotator.annotate(annotated_image, detections)\n",
    "\n",
    "# Display the annotated image\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.imshow(annotated_image)\n",
    "plt.title(\"Object Detection Results\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analysis of Detection Results\n",
    "\n",
    "Let's analyze the results of our object detection to understand what objects were detected, along with their confidence scores and bounding box coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze the detection results\n",
    "print(f\"Number of objects detected: {len(results.boxes)}\")\n",
    "\n",
    "# Extract class names from the model's names dictionary\n",
    "class_names = model.names\n",
    "\n",
    "# Create a summary of detected objects\n",
    "detected_classes = {}\n",
    "for box in results.boxes:\n",
    "    class_id = int(box.cls[0].item())\n",
    "    class_name = class_names[class_id]\n",
    "    confidence = box.conf[0].item()\n",
    "    \n",
    "    if class_name in detected_classes:\n",
    "        detected_classes[class_name].append(round(confidence, 3))\n",
    "    else:\n",
    "        detected_classes[class_name] = [round(confidence, 3)]\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nDetected Objects:\")\n",
    "for class_name, confidences in detected_classes.items():\n",
    "    print(f\"- {class_name}: {len(confidences)} instances, Confidence: {confidences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Object Detection on Additional Images\n",
    "\n",
    "Let's test our model on more images to verify its performance across different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List of image URLs to analyze\n",
    "image_urls = [\n",
    "    \"https://ultralytics.com/images/zidane.jpg\",\n",
    "    \"https://ultralytics.com/images/person.jpg\"\n",
    "]\n",
    "\n",
    "# Function to process and visualize detections\n",
    "def process_image(url, index):\n",
    "    # Download image\n",
    "    image_name = f\"image_{index}.jpg\"\n",
    "    !wget -q -O {image_name} {url}\n",
    "    \n",
    "    # Run detection\n",
    "    results = model(image_name)[0]\n",
    "    \n",
    "    # Prepare visualization\n",
    "    img = cv2.imread(image_name)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    detections = sv.Detections.from_yolov8(results)\n",
    "    annotated_image = bounding_box_annotator.annotate(img, detections)\n",
    "    annotated_image = label_annotator.annotate(annotated_image, detections)\n",
    "    \n",
    "    # Count detections by class\n",
    "    classes = {}\n",
    "    for box in results.boxes:\n",
    "        class_id = int(box.cls[0].item())\n",
    "        class_name = model.names[class_id]\n",
    "        if class_name in classes:\n",
    "            classes[class_name] += 1\n",
    "        else:\n",
    "            classes[class_name] = 1\n",
    "    \n",
    "    # Display results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    \n",
    "    # Original image\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title(\"Original Image\", fontsize=14)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Annotated image\n",
    "    ax2.imshow(annotated_image)\n",
    "    ax2.set_title(f\"Objects Detected: {', '.join([f'{c} ({n})' for c, n in classes.items()])}\", fontsize=14)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results, classes\n",
    "\n",
    "# Process each image\n",
    "for i, url in enumerate(image_urls):\n",
    "    print(f\"\\nImage {i+1}:\")\n",
    "    results, classes = process_image(url, i+1)\n",
    "    print(f\"Detected: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Analysis: Custom Images\n",
    "\n",
    "Now let's analyze any custom images. You can upload your images to the notebook environment and run object detection on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Upload your custom image\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Process uploaded images\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"Processing {filename}...\")\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model(filename)[0]\n",
    "    \n",
    "    # Prepare visualization\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    detections = sv.Detections.from_yolov8(results)\n",
    "    annotated_image = bounding_box_annotator.annotate(img, detections)\n",
    "    annotated_image = label_annotator.annotate(annotated_image, detections)\n",
    "    \n",
    "    # Display images\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    \n",
    "    # Original image\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title(\"Original Image\", fontsize=14)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Annotated image\n",
    "    ax2.imshow(annotated_image)\n",
    "    ax2.set_title(\"Detected Objects\", fontsize=14)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection statistics\n",
    "    print(f\"\\nDetection Results for {filename}:\")\n",
    "    detected_objects = {}\n",
    "    for box in results.boxes:\n",
    "        class_id = int(box.cls[0].item())\n",
    "        class_name = model.names[class_id]\n",
    "        confidence = round(box.conf[0].item(), 2)\n",
    "        \n",
    "        if class_name in detected_objects:\n",
    "            detected_objects[class_name].append(confidence)\n",
    "        else:\n",
    "            detected_objects[class_name] = [confidence]\n",
    "    \n",
    "    # Print summary\n",
    "    for obj_class, confidences in detected_objects.items():\n",
    "        avg_conf = sum(confidences) / len(confidences)\n",
    "        print(f\"- {obj_class}: {len(confidences)} instances, Avg. Confidence: {avg_conf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Applications\n",
    "\n",
    "### 5.1 Video Object Detection\n",
    "\n",
    "YOLO can also perform object detection on videos. Let's implement a function to process videos frame by frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to process a video file\n",
    "def process_video(video_path, output_path, save_output=True, display_frames=False):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties: {width}x{height} @ {fps} fps, {total_frames} frames total\")\n",
    "    \n",
    "    # Create video writer if saving output\n",
    "    if save_output:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Setup annotators\n",
    "    bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "    \n",
    "    # Process frames\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        # Read frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame_count += 1\n",
    "        if frame_count % 10 == 0:  # Print status every 10 frames\n",
    "            print(f\"Processing frame {frame_count}/{total_frames}\")\n",
    "        \n",
    "        # Run detection on frame\n",
    "        results = model(frame)[0]\n",
    "        \n",
    "        # Convert detections to supervision format\n",
    "        detections = sv.Detections.from_yolov8(results)\n",
    "        \n",
    "        # Annotate frame\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # For display\n",
    "        annotated_frame = bounding_box_annotator.annotate(frame_rgb, detections)\n",
    "        annotated_frame = label_annotator.annotate(annotated_frame, detections)\n",
    "        \n",
    "        # Convert back to BGR for video saving\n",
    "        annotated_frame_bgr = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Save frame to output video\n",
    "        if save_output:\n",
    "            out.write(annotated_frame_bgr)\n",
    "        \n",
    "        # Display sample frames if requested\n",
    "        if display_frames and frame_count % 30 == 0:  # Display every 30th frame\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(annotated_frame)\n",
    "            plt.title(f\"Frame {frame_count}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if save_output:\n",
    "        out.release()\n",
    "        \n",
    "    print(f\"Video processing completed. Output saved to: {output_path}\")\n",
    "\n",
    "# Example usage (commented out as we don't have video files in this session)\n",
    "# You can upload a video using files.upload() and then process it\n",
    "\n",
    "# process_video('input_video.mp4', 'output_video.mp4', save_output=True, display_frames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Statistics and Analysis\n",
    "\n",
    "Let's create a function to analyze detection statistics across multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to analyze detection statistics\n",
    "def analyze_detections(results_list, image_names=None):\n",
    "    if image_names is None:\n",
    "        image_names = [f\"Image {i+1}\" for i in range(len(results_list))]\n",
    "    \n",
    "    # Collect statistics\n",
    "    all_stats = []\n",
    "    \n",
    "    for i, results in enumerate(results_list):\n",
    "        image_stats = {\"name\": image_names[i], \"objects\": {}}\n",
    "        \n",
    "        # Count objects by class\n",
    "        for box in results.boxes:\n",
    "            class_id = int(box.cls[0].item())\n",
    "            class_name = model.names[class_id]\n",
    "            confidence = float(box.conf[0].item())\n",
    "            \n",
    "            if class_name not in image_stats[\"objects\"]:\n",
    "                image_stats[\"objects\"][class_name] = {\n",
    "                    \"count\": 0,\n",
    "                    \"confidences\": []\n",
    "                }\n",
    "                \n",
    "            image_stats[\"objects\"][class_name][\"count\"] += 1\n",
    "            image_stats[\"objects\"][class_name][\"confidences\"].append(confidence)\n",
    "        \n",
    "        all_stats.append(image_stats)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    class_counts = {}\n",
    "    \n",
    "    for stats in all_stats:\n",
    "        for class_name, data in stats[\"objects\"].items():\n",
    "            if class_name not in class_counts:\n",
    "                class_counts[class_name] = 0\n",
    "            class_counts[class_name] += data[\"count\"]\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"Detection Statistics:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, stats in enumerate(all_stats):\n",
    "        print(f\"\\n{stats['name']}:\")\n",
    "        for class_name, data in stats[\"objects\"].items():\n",
    "            avg_conf = sum(data[\"confidences\"]) / len(data[\"confidences\"])\n",
    "            print(f\"  - {class_name}: {data['count']} instances, Avg. Confidence: {avg_conf:.2f}\")\n",
    "    \n",
    "    print(\"\\nOverall Statistics:\")\n",
    "    print(\"-\" * 40)\n",
    "    total_objects = sum(class_counts.values())\n",
    "    print(f\"Total objects detected: {total_objects}\")\n",
    "    \n",
    "    # Sort classes by occurrence count\n",
    "    sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    for class_name, count in sorted_classes:\n",
    "        print(f\"  - {class_name}: {count} instances ({count/total_objects*100:.1f}%)\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot top 10 classes\n",
    "    top_classes = sorted_classes[:10] if len(sorted_classes) > 10 else sorted_classes\n",
    "    \n",
    "    classes = [c[0] for c in top_classes]\n",
    "    counts = [c[1] for c in top_classes]\n",
    "    \n",
    "    plt.bar(classes, counts, color='skyblue')\n",
    "    plt.title(\"Top Detected Object Classes\", fontsize=15)\n",
    "    plt.xlabel(\"Object Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return all_stats, class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've demonstrated how to use YOLOv8 for object detection in images. We've covered:\n",
    "\n",
    "1. Setting up and loading the YOLO model\n",
    "2. Performing object detection on images\n",
    "3. Visualizing and analyzing detection results\n",
    "4. Advanced applications like video processing\n",
    "5. Statistical analysis of detections\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To extend this project, you could:\n",
    "- Fine-tune the YOLO model on your own dataset\n",
    "- Implement real-time object detection using a webcam\n",
    "- Develop specific applications based on detection results (e.g., counting people, traffic analysis)\n",
    "- Combine with tracking algorithms for object tracking in videos\n",
    "- Export the model for deployment in production environments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
